{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Emotion_Dataset/text_emotion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "# # Problem \n",
    "# 1) sparcity leads to overfitting\n",
    "# 2) out of vacoblary probelem\n",
    "# 3) no symantic meaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 48212)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of words special case of n-grams ( Good for text classification )\n",
    "\n",
    "# context don't take into accout\n",
    "# sparcity leads to overfitting\n",
    "# oov\n",
    "# ordering not take into account\n",
    "\n",
    "# handle oov problem (don;t throw error)\n",
    "# Simple and intuitive\n",
    "# symantic meaning tak into account ( little)\n",
    "# fix size every time for data unlike one hot\n",
    "# frequecy and \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv  = CountVectorizer() # CountVectorizer(binary= True) --> for sentiment analysis\n",
    "\n",
    "bow = cv.fit_transform(df['content'])\n",
    "\n",
    "# cv.vocabulary_\n",
    "\n",
    "print(bow[0].shape)\n",
    "bow[1].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 227302)\n"
     ]
    }
   ],
   "source": [
    "# Bag of n-gram\n",
    "# e.g shantnu is good / shantnu is not good (have mode distance b/w them)\n",
    "# capture more symentic meaning\n",
    "# Simple and intuitive\n",
    "\n",
    "# for large dataset unigram > bigram > trigram ( time complexity increase)\n",
    "# oov\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv  = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "n_gram = cv.fit_transform(df['content'])\n",
    "\n",
    "print(n_gram[0].toarray().shape)\n",
    "\n",
    "# cv.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.2293639   8.33856319 10.90351255 ... 10.49804744 10.90351255\n",
      "  8.88860953]\n",
      "['00' '000' '000th' ... '½you' '½z' '½ï']\n"
     ]
    }
   ],
   "source": [
    "# Tf - IDF\n",
    "\n",
    "# If word more in doc but not in corpus then that word more weitage\n",
    "# tf - term frequency\n",
    "#IDf - inverse docu frequency (why log --> cause it's change more into less and also stop overpowering of idf)\n",
    "# use in info retirval system\n",
    "\n",
    "# sparcity leads to overfitting\n",
    "# oov\n",
    "# not capture sematic meaning\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "df_2 =  pd.read_csv('Emotion_Dataset/text_emotion.csv')\n",
    "\n",
    "tf.fit_transform(df_2['content']).toarray()\n",
    "\n",
    "print(tf.idf_)\n",
    "print(tf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custome Feature \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
